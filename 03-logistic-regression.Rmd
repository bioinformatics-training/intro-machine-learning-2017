<<<<<<< HEAD
# Linear and non linear (logistic) regression
=======
# Linear and non linear regression and classification {#logistic-regression}
>>>>>>> 634734233afdb3dfc9a40a09faa8cc06d917ce62

<!-- Chris -->


<<<<<<< HEAD

A large component of machine learning involveds either regression analysis or classification analysis. Within regression analysis we wish to find how a particular continuous variable of interest, y, is influenced by another set of variables, x. Conceptually, this involves identifying a function that tells us the value of y given a observed x, for example, how the expression of a particular gene, y_i, changes over time, or as a function of the expression of itse regulators. In general, many applications in ML can be represented in terms of regression, including differential expression analysis and network inference.

Classificaiton algorithms, on the other hand, deal with discrete (categorical) valued outpus. The aim, here is to the ability to learn how different input values, x, map to a particular group (category), and ultimately to assign categories to a new set of observations for which catagories have no yet been assigned.

Within this chapter we will cover linear and nonlinear regression and classification using examples taken from plant datasets infected with pathogens. In particular we will be investigating the expression levels of the model plant Arabidopsis thaliana following infection with the necrotophic pathogen Botrytis cineara. Botrytis cinerea ...

The processed data is available on GEO under accession number GSE39597. Due to time constraints a pre-processed version of data is available from /data/Arabidopsis/Arabidopsis_Botrytis.csv. The dataset is tab delimited text file with the fist columns containig gene names for 163 marker genes. Columns 2 through 49 contain processed gene expression values (see Windram et al. 2012, for full details). The first 24 columns of the data matrix represent contain the control gene expression in Arabidopsis leaves at time points 2h through 48h at 2 hourly intervals; the second 24 columns contain the gene expression levels in Arabidopsis leaves following infection with the necrotophic fungus, Botyris cinerea. 

Excercise 3.1. Load in the data and plot the gene expresison of the genes, to visualise.

```{r echo=T}
genenames <- colnames(D)
geneindex <- 35
D <- read.csv(file = "/Users/christopher_penfold/Desktop/MLCourse/intro-machine-learning/data/Arabidopsis/Arabidopsis_Botrytis_transpose.csv", header = TRUE, sep = ",", row.names=1)
Xs <- seq(from = 2, to = 48, by = 2)
plot(Xs,(D[1:24,geneindex]),type="p",col="black",ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex])
points(Xs,(D[25:nrow(D),geneindex]),type="p",col="red")
```

## Regression {#regression}

One of the simplest models to fit to data involves linear regression. Within linear regression we assume we have a simple model with the following form:

$y = mx + c$

For a given set of data, we will typically have a set of observations $\mathbf{x} = \{x_1,x_2,\ldots,x_n\}$ with a corresponding set of observations,  $\mathbf{y} = \{y_1,y_2,\ldots,y_n\}$. The aim is to infer the parameters $m$ and $c$. Linear regression can be implemented in R via:

```{r echo=F}
lm(AT2G28890~Time, data = D[25:nrow(D),])
```



```{r echo=F}
install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
library(mlbench)
set.seed(1)
```

We can fit the model (and print out a summary) using the following snippet:

```{r echo=T}
lrfit <- train(y~., data=data.frame(x=Xs,y=D[25:nrow(D),geneindex]), method = "lm")
predictedValues<-predict(lrfit)
summary(lrfit)
```

Finally, we can plot the fit to the control dataset as well and plot the outputs:

```{r echo=T}
lrfit2 <- train(y~., data=data.frame(x=Xs,y=D[1:24,geneindex]), method = "lm")
predictedValues2<-predict(lrfit2)

plot(Xs,D[25:nrow(D),geneindex],type="p",col="black",ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex])
points(Xs,D[1:24,geneindex],type="p",col="red")
points(Xs,predictedValues,type="l",col="black")
points(Xs,predictedValues2,type="l",col="red")
```

For this particular gene linear regression seems to have done a reasonable job at identifying how the gene expression changes over time. In general, however, linear regression will not necesarily provide a good fit for all genes.

Excercise 3.2. Try fitting to some other genes to identify some whose expression profile doesn't change linearly over time.

Excercise 3.3. Linear regression can generally be applied for any number of variables. A notable example, would be to regress the expression pattern of a gene against putative regulators.

```{r echo=T}
lrfit3 <- train(y~., data=data.frame(x=D[1:24,1:10],y=D[1:24,geneindex]), method = "lm")
```

## Gaussian process regression

Overfitting.

Define nonlinear. Show with some examples. May need to bypass standard definitions due to time constraints. These can be supplementary along with the maths. Here focus on use. 

$y = f(x)$

where $f(x)$ represents some potentially nonlinear function. Gaussian process regression represents a way to estimate a distribution over functions ...

As a nonparemtric approach, Gaussian processes essentially encode all ..

https://www.r-bloggers.com/gaussian-process-regression-with-r/

$f_* = k_*^\top(K)^{-1} y$
$f_* = k(x_*,x_*)^{-1} - k_*^\top (K)^{-1} k_*$

Although the most complete GP toolboxes can be found in Matlab and Python, some GP toolboxes exist in R. In particular, notable resources such as https://www.r-bloggers.com/gaussian-process-regression-with-r/First have implemented a variety of of the gpml package. To better understand GPs we first load in some required packages.

```{r echo=T}
require(MASS)
require(plyr)
require(reshape2)
require(ggplot2)
```

Following https://www.r-bloggers.com/gaussian-process-regression-with-r/ we can define an R function to calculate the covariance function:

$C(x,x^\prime) = \sigma^2 exp\biggl{(}\frac{(x-x^\prime)^2}{2l^2}\biggr{)}$. We can implement this:

```{r echo=T}
calcSigma <- function(X1,X2,l=1,sig=1) {
  Sigma <- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1))
  for (i in 1:nrow(Sigma)) {
    for (j in 1:ncol(Sigma)) {
      Sigma[i,j] <- sig*exp(-0.5*(abs(X1[i]-X2[j])/l)^2)
    }
  }
  return(Sigma)
}
```

To better understand GPs, we can generate samples from the GP prior. In the example below, we generate three sample functions:

```{r echo=T}
x.star <- seq(-5,5,len=500) #Define a set of points at which to evaluate the functions
sigma  <- calcSigma(x.star,x.star) #Evaluate the covariance function at those locations, to give the covariance matrix.
y1 <- mvrnorm(1, rep(0, length(x.star)), sigma)
y2 <- mvrnorm(1, rep(0, length(x.star)), sigma)
y3 <- mvrnorm(1, rep(0, length(x.star)), sigma)
```

From the plot we should get a rough feel for what is going on (although it may help to generate a few more sample functions). In genearl, the GP specifies a distribution over different functions. The type of function is specfied by the covariance function, for example, the squared exponential covariannce function generates very smooth functions. A variety of other covariance functions exist, and can be found here: 

Excercise 3.4 (optional): Try implementing another covariance function and generating samples from the GP prior.

So we can generate samples from the prior, but what about inference? What is the GP doing? Essentially, the GP is representing the data in terms of the observation data and the hyperperameters. For simplicity, let's assume we have a function $y = sin(x)$, and we have some observations. For example, we have an observation at $x=-2$.

```{r echo=T}
f <- data.frame(x=c(-2),
                y=sin(c(-2)))
```

We can infer a posterior GP:

```{r echo=T}
x <- f$x
k.xx <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
k.xsxs <- calcSigma(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

Excercise 3.5 (optional): Try plotting some sample function from the posterior GP.

We can start to add more observations. Here's what the posterior fit looks like if we include 4 observations (at x in [-4,-2,0,1]):

```{r echo=F}
f <- data.frame(x=c(-4,-2,0,1),
                y=sin(c(-4,-2,0,1)))
x <- f$x
k.xx <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
k.xsxs <- calcSigma(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

And with 7 observations:

```{r echo=F}
f <- data.frame(x=c(-4,-3,-2,-1,0,1,2),
                y=sin(c(-4,-3,-2,-1,0,1,2)))
x <- f$x
k.xx <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
k.xsxs <- calcSigma(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

We can see that the posterior GP starts to resemble the true underlying function very well. Over most of the x axis the mean of the GP lies very close to the true function and, perhaps more importantly, we have an treatment for the uncertainty. With the GP we can see where the model is relatively sure of the behaviour of the function, and we can see where the model is uncertain. We might, for example want to get observations at x = 4, where the posterior errorbars are greatest.

Another key aspect of GP regression is the ability to evaluate marginal likelihood, otherwise referred to as the "model evidence". We can calculate the marginal likelihood using the snippet of code below:

$f(x) \sim \mathcal{GP}(m(x), k(x,x^\prime))í°ƒ.$


$\ln p(\mathbf{y}|\mathbf{x}) = -\frac{1}{2}\mathbf{y}^\top (K)^{-1} \mathbf{y} -\frac{1}{2} \ln |K| - \frac{n}{2}\ln 2\pi$

```{r echo=T}
calcML <- function(f,l=1,sig=1) {
  f2 <- t(f)
  yt <- f2[2,]
  y  <- f[,2]
  K <- calcSigma(f[,1],f[,1],l,sig)
  ML <- -0.5*yt%*%ginv(K)%*%y -0.5*log(det(K)) -(length(f[,1])/2)*log(2*pi);
  return(ML)
}
```

Crucially, by optimising the marginal we can automatically select the hyperparameters.

```{r echo=T}
par <- seq(0.1,10,by=0.1)
ML <- matrix(rep(0, length(par)), nrow=length(par))
for(i in 1:length(par)) {
    ML[i] <- calcML(f,par[i],1)
}
plot(par, ML,type = 'l',col="red")
par[which(ML==max(ML))] #Print out the MAP value
```

Excercise 3.1: Try fitting a Gaussian process to one of the gene expression profiles in the Botrytis dataset. Hint: identify a gene with good dynamics.

### Model Selection

The marginal likelihood can additionally be used as a basis for selecting models. For example studies by Stegle et al. (2010) used Gaussian processes to identify differential expression between two time series e.g., between a control time series and an infection time series. 

Differential expression analysis is a way of determininig whether two sets of data are different. For example, if one measured the expression of a set of genes in two conditions, you could use an appropriate statistical test to determin whether the expression of those genes varied significantly in the two conditions. The most often used test are either Student's t-test or rank based test. Both tests however, are not appropriate for time series data, in which we have temporal information. 

Gaussian processes represent a useufl way of test for the differences in genes expression for time series observations. Here we are using the machine learning approaches to fit a model, first to the one time series, the to the second time series, and finally to a combination of the time series. A related example of this kind of approach was introduced by Stegle et al. (2010), with extensions introduced by Rattray et al. (2016) and Penfold, Sybirna et al. (2017).

Excercise 3.1: Use GPs for model selection.

### Differential expression analysis 

The application in the previous section. We have 

Stegle et al. Rattray et al.

```{r echo=T}
install.packages("devtools")
library(devtools)
install_github("ManchesterBioinference/DEtime")
import(DEtime)
library(DEtime)
```


```{r echo=T}
res <- DEtime_infer(ControlTimes = Xs, ControlData = D[1:24,2], PerturbedTimes = Xs, PerturbedData = D[25:48,2])
print_DEtime
plot_DEtime(res)
```



res <- DEtime_infer(ControlTimes = Xs, ControlData = D[1:24,2], PerturbedTimes = Xs, PerturbedData = D[25:48,2])
print_DEtime
plot_DEtime(res)

res_rank <- DEtime_rank(ControlTimes = ControlTimes, ControlData = ControlData, PerturbedTimes, PerturbedData=PerturbedData, savefile=TRUE)
idx <- which(res_rank[,2]>1)


res <- DEtime_infer(ControlTimes = Xs, ControlData = t(D[1:24,]), PerturbedTimes = Xs, PerturbedData = t(D[25:48,]))
print_DEtime
plot_DEtime(res)

hist(as.numeric(res$result[,2]))

hist(as.numeric(res$result[,2]),breaks=20)

### inport simulated dataset
data(SimulatedData)

### go on with the perturbation time point inference
res <- DEtime_infer(ControlTimes = ControlTimes, ControlData = ControlData, PerturbedTimes = PerturbedTimes, PerturbedData = PerturbedData)

### Print a summary of the results
print_DEtime(res)
### plot results for all the genes
plot_DEtime(res)

## Classificaiton

Importance of classifiers.

Infer regulators that identify infection levels in plants.

## Logistic regression

library(caret)

In linear regression we tried to predict the value of y(i)
 for the i
â€˜th example x(i)
 using a linear function y=hÎ¸(x)=Î¸âŠ¤x.
. This is clearly not a great solution for predicting binary-valued labels (y(i)âˆˆ{0,1})
. In logistic regression we use a different hypothesis class to try to predict the probability that a given example belongs to the â€œ1â€ class versus the probability that it belongs to the â€œ0â€ class. Specifically, we will try to learn a function of the form:

## GP classification

Example GP regression using kernlab

=======
## Exercises

Solutions to exercises can be found in appendix \@ref(solutions-logistic-regression).
>>>>>>> 634734233afdb3dfc9a40a09faa8cc06d917ce62
